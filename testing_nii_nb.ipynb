{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yufan/anaconda3/envs/pruning_app/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torchvision.models import vgg11_bn, VGG11_BN_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.build_model import all_classifiers\n",
    "model = all_classifiers[\"resnet18\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|     Name     | Param Num |\n",
      "+--------------+-----------+\n",
      "|  features.0  |   1.79 K  |\n",
      "|  features.1  |    128    |\n",
      "|  features.4  |  73.86 K  |\n",
      "|  features.5  |    256    |\n",
      "|  features.8  |  295.17 K |\n",
      "|  features.9  |    512    |\n",
      "| features.11  |  590.08 K |\n",
      "| features.12  |    512    |\n",
      "| features.15  |   1.18 M  |\n",
      "| features.16  |   1.02 K  |\n",
      "| features.18  |   2.36 M  |\n",
      "| features.19  |   1.02 K  |\n",
      "| features.22  |   2.36 M  |\n",
      "| features.23  |   1.02 K  |\n",
      "| features.25  |   2.36 M  |\n",
      "| features.26  |   1.02 K  |\n",
      "| classifier.0 |   2.10 M  |\n",
      "| classifier.3 |  16.78 M  |\n",
      "| classifier.6 |  40.97 K  |\n",
      "+--------------+-----------+\n",
      "Total Param Num: 28.15 M\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def simplify_number(num):\n",
    "    if num >= 1_000_000:\n",
    "        return f\"{num / 1_000_000:.2f} M\"\n",
    "    elif num >= 1_000:\n",
    "        return f\"{num / 1_000:.2f} K\"\n",
    "    else:\n",
    "        return str(num)\n",
    "\n",
    "# Test the function\n",
    "# print(simplify_number(28149514))\n",
    "\n",
    "param_num = {}\n",
    "layer_spec = {}\n",
    "total_param = 0\n",
    "for name, param in model.named_parameters():\n",
    "    n = name.split(\".\")[:2]\n",
    "    n = \".\".join(n)\n",
    "    if n not in param_num:\n",
    "        param_num[n] = param.numel()\n",
    "    else:\n",
    "        param_num[n] += param.numel()\n",
    "    total_param += param.numel()\n",
    "\n",
    "table = PrettyTable([\"Name\", \"Param Num\"])\n",
    "for name, num in param_num.items():\n",
    "    table.add_row([name, simplify_number(num)])\n",
    "print(table)\n",
    "print(f\"Total Param Num: {simplify_number(total_param)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------+\n",
      "|          Name         |      Spec     |\n",
      "+-----------------------+---------------+\n",
      "|         conv1         |  Conv 3x3, 64 |\n",
      "|     layer1.0.conv1    |  Conv 3x3, 64 |\n",
      "|     layer1.0.conv2    |  Conv 3x3, 64 |\n",
      "|     layer1.1.conv1    |  Conv 3x3, 64 |\n",
      "|     layer1.1.conv2    |  Conv 3x3, 64 |\n",
      "|     layer2.0.conv1    | Conv 3x3, 128 |\n",
      "|     layer2.0.conv2    | Conv 3x3, 128 |\n",
      "| layer2.0.downsample.0 | Conv 1x1, 128 |\n",
      "|     layer2.1.conv1    | Conv 3x3, 128 |\n",
      "|     layer2.1.conv2    | Conv 3x3, 128 |\n",
      "|     layer3.0.conv1    | Conv 3x3, 256 |\n",
      "|     layer3.0.conv2    | Conv 3x3, 256 |\n",
      "| layer3.0.downsample.0 | Conv 1x1, 256 |\n",
      "|     layer3.1.conv1    | Conv 3x3, 256 |\n",
      "|     layer3.1.conv2    | Conv 3x3, 256 |\n",
      "|     layer4.0.conv1    | Conv 3x3, 512 |\n",
      "|     layer4.0.conv2    | Conv 3x3, 512 |\n",
      "| layer4.0.downsample.0 | Conv 1x1, 512 |\n",
      "|     layer4.1.conv1    | Conv 3x3, 512 |\n",
      "|     layer4.1.conv2    | Conv 3x3, 512 |\n",
      "|           fc          |   Linear, 10  |\n",
      "+-----------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "layer_spec = {}\n",
    "spec_tab = PrettyTable([\"Name\", \"Spec\"])\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        layer_spec[name] = module\n",
    "        spec_tab.add_row([name, f\"Conv {module.kernel_size[0]}x{module.kernel_size[1]}, {module.out_channels}\"])\n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        spec_tab.add_row([name, f\"Linear, {module.out_features}\"])\n",
    "tab = spec_tab.get_formatted_string()\n",
    "print(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------+\n",
      "|       Modules       | Parameters |\n",
      "+---------------------+------------+\n",
      "|  features.0.weight  |    1728    |\n",
      "|   features.0.bias   |     64     |\n",
      "|  features.1.weight  |     64     |\n",
      "|   features.1.bias   |     64     |\n",
      "|  features.4.weight  |   73728    |\n",
      "|   features.4.bias   |    128     |\n",
      "|  features.5.weight  |    128     |\n",
      "|   features.5.bias   |    128     |\n",
      "|  features.8.weight  |   294912   |\n",
      "|   features.8.bias   |    256     |\n",
      "|  features.9.weight  |    256     |\n",
      "|   features.9.bias   |    256     |\n",
      "|  features.11.weight |   589824   |\n",
      "|   features.11.bias  |    256     |\n",
      "|  features.12.weight |    256     |\n",
      "|   features.12.bias  |    256     |\n",
      "|  features.15.weight |  1179648   |\n",
      "|   features.15.bias  |    512     |\n",
      "|  features.16.weight |    512     |\n",
      "|   features.16.bias  |    512     |\n",
      "|  features.18.weight |  2359296   |\n",
      "|   features.18.bias  |    512     |\n",
      "|  features.19.weight |    512     |\n",
      "|   features.19.bias  |    512     |\n",
      "|  features.22.weight |  2359296   |\n",
      "|   features.22.bias  |    512     |\n",
      "|  features.23.weight |    512     |\n",
      "|   features.23.bias  |    512     |\n",
      "|  features.25.weight |  2359296   |\n",
      "|   features.25.bias  |    512     |\n",
      "|  features.26.weight |    512     |\n",
      "|   features.26.bias  |    512     |\n",
      "| classifier.0.weight |  2097152   |\n",
      "|  classifier.0.bias  |    4096    |\n",
      "| classifier.3.weight |  16777216  |\n",
      "|  classifier.3.bias  |    4096    |\n",
      "| classifier.6.weight |   40960    |\n",
      "|  classifier.6.bias  |     10     |\n",
      "+---------------------+------------+\n",
      "Total Trainable Params: 28149514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28149514"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# To get the parameter count of each layer like Keras, PyTorch has model.named_parameters() that returns an iterator over both the parameter name and the parameter itself. Example:\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\", ])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the trainer and evaluaF.nll_loss(output, target, reduction='sum').item()tor\n",
    "def trainer(model, optimizer, criterion, train_loader):\n",
    "    # training the model\n",
    "    model.train()\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def evaluator(model, test_loader):\n",
    "    # evaluating the model accuracy and average test loss\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_dataset_length = 64\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += loss(output, target)\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            break\n",
    "    test_loss /= test_dataset_length\n",
    "    accuracy = 100. * correct / test_dataset_length\n",
    "    print('Average test loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(test_loss, correct, test_dataset_length, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 0.3423, Accuracy: 0/64 (0%)\n"
     ]
    }
   ],
   "source": [
    "evaluator(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [{\n",
    "    'op_types': ['Conv2d', \"Linear\"],\n",
    "    'exclude_op_names': ['classifier.6'],\n",
    "    'sparse_ratio': 0.5\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_nni_wrapper): ModuleWrapper(module=Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), module_name=features.0)\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_nni_wrapper): ModuleWrapper(module=Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), module_name=features.4)\n",
      "    )\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_nni_wrapper): ModuleWrapper(module=Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), module_name=features.8)\n",
      "    )\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_nni_wrapper): ModuleWrapper(module=Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), module_name=features.11)\n",
      "    )\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Conv2d(\n",
      "      256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_nni_wrapper): ModuleWrapper(module=Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), module_name=features.15)\n",
      "    )\n",
      "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_nni_wrapper): ModuleWrapper(module=Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), module_name=features.18)\n",
      "    )\n",
      "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (22): Conv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_nni_wrapper): ModuleWrapper(module=Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), module_name=features.22)\n",
      "    )\n",
      "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_nni_wrapper): ModuleWrapper(module=Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), module_name=features.25)\n",
      "    )\n",
      "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(\n",
      "      in_features=512, out_features=4096, bias=True\n",
      "      (_nni_wrapper): ModuleWrapper(module=Linear(in_features=512, out_features=4096, bias=True), module_name=classifier.0)\n",
      "    )\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(\n",
      "      in_features=4096, out_features=4096, bias=True\n",
      "      (_nni_wrapper): ModuleWrapper(module=Linear(in_features=4096, out_features=4096, bias=True), module_name=classifier.3)\n",
      "    )\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from nni.compression.pruning import L1NormPruner, L2NormPruner\n",
    "pruner = L2NormPruner(model, config_list)\n",
    "\n",
    "# show the wrapped model structure, `PrunerModuleWrapper` have wrapped the layers that configured in the config_list.\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.11  sparsity :  0.5\n",
      "features.0  sparsity :  0.5\n",
      "features.18  sparsity :  0.5\n",
      "features.22  sparsity :  0.5\n",
      "features.4  sparsity :  0.5\n",
      "features.8  sparsity :  0.5\n",
      "features.15  sparsity :  0.5\n",
      "features.25  sparsity :  0.5\n",
      "classifier.3  sparsity :  0.5\n",
      "classifier.0  sparsity :  0.5\n"
     ]
    }
   ],
   "source": [
    "# compress the model and generate the masks\n",
    "_, masks = pruner.compress()\n",
    "# show the masks sparsity\n",
    "for name, mask in masks.items():\n",
    "    print(name, ' sparsity : ', '{:.2}'.format(mask['weight'].sum() / mask['weight'].numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-18 14:12:25] \u001b[32mStart to speedup the model...\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mResolve the mask conflict before mask propagate...\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mdim0 sparsity: 0.500000\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mdim1 sparsity: 0.000000\u001b[0m\n",
      "0 Filter\n",
      "[2023-09-18 14:12:25] \u001b[32mdim0 sparsity: 0.500000\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mdim1 sparsity: 0.000000\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mInfer module masks...\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate original variables\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for placeholder: x, output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_0, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_1, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_2, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_3, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_4, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_5, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_6, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_7, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_8, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_9, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_10, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_11, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_12, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_13, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_14, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_15, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_16, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_17, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_18, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_19, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_20, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_21, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_22, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_23, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_24, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_25, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_26, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_27, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: features_28, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: avgpool, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_method: size, \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_method: view, output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: classifier_0, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: classifier_1, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: classifier_2, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: classifier_3, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: classifier_4, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: classifier_5, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for call_module: classifier_6, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mPropagate variables for output: output, output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct sparsity...\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for placeholder: x, output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_0, weight:  0.5000 bias:  0.5000 , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_1, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_2, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_3, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_4, weight:  0.5000 bias:  0.5000 , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_5, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_6, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_7, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_8, weight:  0.5000 bias:  0.5000 , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_9, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_10, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_11, weight:  0.5000 bias:  0.5000 , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_12, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_13, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_14, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_15, weight:  0.5000 bias:  0.5000 , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_16, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_17, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_18, weight:  0.5000 bias:  0.5000 , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_19, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_20, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_21, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_22, weight:  0.5000 bias:  0.5000 , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_23, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_24, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_25, weight:  0.5000 bias:  0.5000 , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_26, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_27, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: features_28, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: avgpool, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_method: size, \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_method: view, output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: classifier_0, weight:  0.5000 bias:  0.5000 , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: classifier_1, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: classifier_2, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: classifier_3, weight:  0.5000 bias:  0.5000 , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: classifier_4, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: classifier_5, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for call_module: classifier_6, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate direct mask for output: output, output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect sparsity...\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for output: output, output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: classifier_6, , output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: classifier_5, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: classifier_4, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: classifier_3, weight:  0.7500 bias:  0.5000 , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: classifier_2, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: classifier_1, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: classifier_0, weight:  0.7500 bias:  0.5000 , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_method: view, output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_method: size, \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: avgpool, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_28, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_27, , output mask:  0.5498 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_26, , output mask:  0.5498 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_25, weight:  0.7617 bias:  0.5000 , output mask:  0.5498 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_24, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_23, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_22, weight:  0.7500 bias:  0.5000 , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_21, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_20, , output mask:  0.5516 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_19, , output mask:  0.5516 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_18, weight:  0.7500 bias:  0.5000 , output mask:  0.5516 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_17, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_16, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_15, weight:  0.7500 bias:  0.5000 , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_14, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_13, , output mask:  0.5480 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_12, , output mask:  0.5480 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_11, weight:  0.7500 bias:  0.5000 , output mask:  0.5480 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_10, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_9, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_8, weight:  0.7500 bias:  0.5000 , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_7, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_6, , output mask:  0.5502 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_5, , output mask:  0.5502 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_4, weight:  0.7500 bias:  0.5000 , output mask:  0.5502 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_3, , output mask:  0.5000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_2, , output mask:  0.5491 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_1, , output mask:  0.5491 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for call_module: features_0, weight:  0.5000 bias:  0.5000 , output mask:  0.5491 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mUpdate indirect mask for placeholder: x, output mask:  0.0000 \u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mResolve the mask conflict after mask propagate...\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mdim0 sparsity: 0.500000\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mdim1 sparsity: 0.499331\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[33mWARNING: both dim0 and dim1 masks found.\u001b[0m\n",
      "0 Filter\n",
      "[2023-09-18 14:12:25] \u001b[32mdim0 sparsity: 0.500000\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mdim1 sparsity: 0.499331\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[33mWARNING: both dim0 and dim1 masks found.\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mReplace compressed modules...\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_0, op_type: Conv2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace conv2d with in_channels: 3, out_channels: 32\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_1, op_type: BatchNorm2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace batchnorm2d with num_features: 32\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_2, op_type: ReLU)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_3, op_type: MaxPool2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_4, op_type: Conv2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace conv2d with in_channels: 32, out_channels: 64\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_5, op_type: BatchNorm2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace batchnorm2d with num_features: 64\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_6, op_type: ReLU)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_7, op_type: MaxPool2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_8, op_type: Conv2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace conv2d with in_channels: 64, out_channels: 128\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_9, op_type: BatchNorm2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace batchnorm2d with num_features: 128\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_10, op_type: ReLU)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_11, op_type: Conv2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace conv2d with in_channels: 128, out_channels: 128\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_12, op_type: BatchNorm2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace batchnorm2d with num_features: 128\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_13, op_type: ReLU)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_14, op_type: MaxPool2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_15, op_type: Conv2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace conv2d with in_channels: 128, out_channels: 256\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_16, op_type: BatchNorm2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace batchnorm2d with num_features: 256\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_17, op_type: ReLU)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_18, op_type: Conv2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace conv2d with in_channels: 256, out_channels: 256\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_19, op_type: BatchNorm2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace batchnorm2d with num_features: 256\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_20, op_type: ReLU)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_21, op_type: MaxPool2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_22, op_type: Conv2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace conv2d with in_channels: 256, out_channels: 256\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_23, op_type: BatchNorm2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace batchnorm2d with num_features: 256\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_24, op_type: ReLU)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_25, op_type: Conv2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace conv2d with in_channels: 256, out_channels: 256\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_26, op_type: BatchNorm2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace batchnorm2d with num_features: 256\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_27, op_type: ReLU)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: features_28, op_type: MaxPool2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: avgpool, op_type: AdaptiveAvgPool2d)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: classifier_0, op_type: Linear)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace linear with new in_features: 256, out_features: 2048\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: classifier_1, op_type: ReLU)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: classifier_2, op_type: Dropout)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: classifier_3, op_type: Linear)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace linear with new in_features: 2048, out_features: 2048\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: classifier_4, op_type: ReLU)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: classifier_5, op_type: Dropout)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace module (name: classifier_6, op_type: Linear)\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mreplace linear with new in_features: 2048, out_features: 10\u001b[0m\n",
      "[2023-09-18 14:12:25] \u001b[32mSpeedup done.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=2048, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to unwrap the model, if the model is wrapped before speedup\n",
    "pruner.unwrap_model()\n",
    "\n",
    "# speedup the model, for more information about speedup, please refer :doc:`pruning_speedup`.\n",
    "from nni.compression.speedup import ModelSpeedup\n",
    "\n",
    "ModelSpeedup(model, torch.rand(64, 3, 32, 32).to(device), masks).speedup_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (23): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=2048, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), 1e-2)\n",
    "for epoch in range(17):\n",
    "    trainer(model, optimizer, criterion, trainloader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 0.0092, Accuracy: 53/64 (83%)\n"
     ]
    }
   ],
   "source": [
    "evaluator(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SEML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
